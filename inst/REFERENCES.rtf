{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww15580\viewh13840\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf0 \expnd0\expndtw0\kerning0
\{\
	"name": "RuntimeError",\
	"message": "The following operation failed in the TorchScript interpreter.\\nTraceback of TorchScript (most recent call last):\\n  File \\"/Users/davidvandijcke/Dropbox (University of Michigan)/rdd/code/fdd/src/primaldual_torch.py\\", line 95, in forward\\n            if iter%10 == 0:\\n                h_un = u.detach().clone()\\n            u, ubar = self.clipping(p1, p2, p3, u, tauu, h, w, nc, l) # project onto set C\\n                      ~~~~~~~~~~~~~ <--- HERE\\n            if iter%10 == 0:\\n                h_u = u.detach().clone()\\n  File \\"/Users/davidvandijcke/Dropbox (University of Michigan)/rdd/code/fdd/src/primaldual_torch.py\\", line 238, in clipping\\n    def clipping(self, p1, p2, p3, u, tauu, h : int, w : int, nc : int, l : int):\\n        temp = u.detach().clone()\\n        d1 = torch.cat((p1[:-1,:,:,:], torch.zeros(1,w,nc,l)), dim=0) - torch.cat((torch.zeros(1,w,nc,l), p1[:-1,:,:,:]), dim=0)\\n             ~~~~~~~~~ <--- HERE\\n        d2 = torch.cat((p2[:,:-1,:,:], torch.zeros(h,1,nc,l)), dim=1) - torch.cat((torch.zeros(h,1,nc,l), p2[:,:-1,:,:]), dim=1)\\n        d3 = torch.cat((p3[:,:,:,:-1], torch.zeros(h,w,nc,1)), dim=3) - torch.cat((torch.zeros(h,w,nc,1), p3[:,:,:,:-1]), dim=3)\\nRuntimeError: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu\\n",\
	"stack": "\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\\n\\u001b[0;31mRuntimeError\\u001b[0m                              Traceback (most recent call last)\\nCell \\u001b[0;32mIn[4], line 46\\u001b[0m\\n\\u001b[1;32m     40\\u001b[0m nu \\u001b[39m=\\u001b[39m torch\\u001b[39m.\\u001b[39mtensor(\\u001b[39m0.001\\u001b[39m)\\n\\u001b[1;32m     41\\u001b[0m \\u001b[39m# repeats = 1000\\u001b[39;00m\\n\\u001b[1;32m     42\\u001b[0m \\u001b[39m# level = 16\\u001b[39;00m\\n\\u001b[1;32m     43\\u001b[0m \\u001b[39m# lmbda = 1\\u001b[39;00m\\n\\u001b[1;32m     44\\u001b[0m \\u001b[39m# nu = 0.001\\u001b[39;00m\\n\\u001b[0;32m---> 46\\u001b[0m u \\u001b[39m=\\u001b[39m scripted_primal_dual(f, repeats, level, lmbda, nu)\\n\\nFile \\u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\\u001b[0m, in \\u001b[0;36mModule._call_impl\\u001b[0;34m(self, *input, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1186\\u001b[0m \\u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\\u001b[39;00m\\n\\u001b[1;32m   1187\\u001b[0m \\u001b[39m# this function, and just call forward.\\u001b[39;00m\\n\\u001b[1;32m   1188\\u001b[0m \\u001b[39mif\\u001b[39;00m \\u001b[39mnot\\u001b[39;00m (\\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39m_backward_hooks \\u001b[39mor\\u001b[39;00m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39m_forward_hooks \\u001b[39mor\\u001b[39;00m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39m_forward_pre_hooks \\u001b[39mor\\u001b[39;00m _global_backward_hooks\\n\\u001b[1;32m   1189\\u001b[0m         \\u001b[39mor\\u001b[39;00m _global_forward_hooks \\u001b[39mor\\u001b[39;00m _global_forward_pre_hooks):\\n\\u001b[0;32m-> 1190\\u001b[0m     \\u001b[39mreturn\\u001b[39;00m forward_call(\\u001b[39m*\\u001b[39;49m\\u001b[39minput\\u001b[39;49m, \\u001b[39m*\\u001b[39;49m\\u001b[39m*\\u001b[39;49mkwargs)\\n\\u001b[1;32m   1191\\u001b[0m \\u001b[39m# Do not call functions when jit is used\\u001b[39;00m\\n\\u001b[1;32m   1192\\u001b[0m full_backward_hooks, non_full_backward_hooks \\u001b[39m=\\u001b[39m [], []\\n\\n\\u001b[0;31mRuntimeError\\u001b[0m: The following operation failed in the TorchScript interpreter.\\nTraceback of TorchScript (most recent call last):\\n  File \\"/Users/davidvandijcke/Dropbox (University of Michigan)/rdd/code/fdd/src/primaldual_torch.py\\", line 95, in forward\\n            if iter%10 == 0:\\n                h_un = u.detach().clone()\\n            u, ubar = self.clipping(p1, p2, p3, u, tauu, h, w, nc, l) # project onto set C\\n                      ~~~~~~~~~~~~~ <--- HERE\\n            if iter%10 == 0:\\n                h_u = u.detach().clone()\\n  File \\"/Users/davidvandijcke/Dropbox (University of Michigan)/rdd/code/fdd/src/primaldual_torch.py\\", line 238, in clipping\\n    def clipping(self, p1, p2, p3, u, tauu, h : int, w : int, nc : int, l : int):\\n        temp = u.detach().clone()\\n        d1 = torch.cat((p1[:-1,:,:,:], torch.zeros(1,w,nc,l)), dim=0) - torch.cat((torch.zeros(1,w,nc,l), p1[:-1,:,:,:]), dim=0)\\n             ~~~~~~~~~ <--- HERE\\n        d2 = torch.cat((p2[:,:-1,:,:], torch.zeros(h,1,nc,l)), dim=1) - torch.cat((torch.zeros(h,1,nc,l), p2[:,:-1,:,:]), dim=1)\\n        d3 = torch.cat((p3[:,:,:,:-1], torch.zeros(h,w,nc,1)), dim=3) - torch.cat((torch.zeros(h,w,nc,1), p3[:,:,:,:-1]), dim=3)\\nRuntimeError: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu\\n"\
\}}