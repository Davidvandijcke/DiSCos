remove,packages("devtools")
remove.packages("devtools")
remove.packages("usethis")
install.packages("devtools")
unlink("vignettes/Dube2019_cache", recursive = TRUE)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
DiSCo
DiSCo::DiSCo
devtools::load_all()
M = 1000
G = 1000
num.cores = 5
permutation = FALSE
CI = TRUE
boots = 1000
cl = 0.95
CI_periods = NULL
CI_placebo=TRUE
graph = TRUE
qmethod=NULL
# make sure we have a data table
df <- as.data.table(df)
df <- copy(dube)
# if the below gives issues it's the knit cache...
results <- DiSCo(dube, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
M = 1000
G = 1000
num.cores = 5
permutation = FALSE
CI = TRUE
boots = 1000
cl = 0.95
CI_periods = NULL
CI_placebo=TRUE
graph = TRUE
qmethod=NULL
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
id_col.target <- 2
t0 <- 2003
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(DiSCo)
# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
devtools::load_all()
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
M = 1000
G = 1000
num.cores = 5
permutation = FALSE
CI = TRUE
boots = 1000
cl = 0.95
CI_periods = NULL
CI_placebo=TRUE
graph = TRUE
qmethod=NULL
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
# create a column for the normalized time period
t_min <- min(df$time_col)
df[, t_col := time_col - t_min + 1]
T0 <- unique(df[time_col == t0]$t_col)  - 1
T_max <- max(df$t_col)
# create a list to store the results for each period
results.periods <- list()
evgrid = seq(from=0,to=1,length.out=M+1)
# run the main function in parallel for each period
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M,
G = G, T0 = T0, mc.cores = num.cores, qmethod=qmethod)
# turn results.periods into a named list where the name is the period
names(results.periods) <- as.character(periods)
yy <- 1
# target
target <- df[(id_col == id_col.target) & (t_col == yy)]$y_col
# generate list where each element contains a list of all micro-level outcomes for a control unit
controls <- list()
j <- 1
controls.id <- unique(df[id_col != id_col.target]$id_col)
for (id in controls.id) {
controls[[j]] <- df[(id_col == id) & (t_col == yy)]$y_col
j <- j + 1
}
# check whether problem undetermined
if (length(controls[[1]]) < length(controls)) {
stop("Problem undetermined: number of data points is smaller than number of weights")
}
# evaluating the quantile functions on the grid "evgrid":
controls.q <- matrix(0,nrow = length(evgrid), ncol=length(controls))
for (jj in 1:length(controls)){
# controls.q[,jj] <- mapply(myquant, evgrid, MoreArgs = list(X=controls[[jj]]))
controls.q[,jj] <- myQuant(controls[[jj]], evgrid, qmethod)
}
# sample grid
grid <- list(grid.min = NA, grid.max = NA, grid.rand = NA, grid.ord = NA)
grid[c("grid.min", "grid.max", "grid.rand", "grid.ord")] <- getGrid(target, controls, G) # TODO: this can be done just once
if (yy <= T0) { # only get weights for pre-treatment periods
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M, qmethod=qmethod)
# obtaining the optimal weights for the mixture of distributions method
mixture <- DiSCo_mixture(controls, target, grid$grid.min, grid$grid.max, grid$grid.rand)
} else {
DiSCo_res_weights <- NA
mixture <- list(weights.opt = NA, distance.opt = NA, mean = NA)
}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(DiSCo)
# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
devtools::load_all()
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
df <- copy(dube)
# if the below gives issues it's the knit cache...
results <- DiSCo(dube, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
devtools::install()
unlink("vignettes/Dube2019_cache", recursive = TRUE)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(DiSCo)
df <- copy(dube)
devtools::install)()
devtools::install()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(DiSCo)
# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
devtools::load_all()
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(DiSCo)
# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
devtools::load_all()
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
df <- copy(dube)
# if the below gives issues it's the knit cache...
results <- DiSCo(dube, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
M = 1000
G = 1000
num.cores = 5
permutation = FALSE
CI = TRUE
boots = 1000
cl = 0.95
CI_periods = NULL
CI_placebo=TRUE
graph = TRUE
qmethod=NULL
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
# create a column for the normalized time period
t_min <- min(df$time_col)
df[, t_col := time_col - t_min + 1]
T0 <- unique(df[time_col == t0]$t_col)  - 1
T_max <- max(df$t_col)
# create a list to store the results for each period
results.periods <- list()
evgrid = seq(from=0,to=1,length.out=M+1)
# run the main function in parallel for each period
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M,
G = G, T0 = T0, mc.cores = num.cores, qmethod=qmethod)
# turn results.periods into a named list where the name is the period
names(results.periods) <- as.character(periods)
yy <- 1
# target
target <- df[(id_col == id_col.target) & (t_col == yy)]$y_col
# generate list where each element contains a list of all micro-level outcomes for a control unit
controls <- list()
j <- 1
controls.id <- unique(df[id_col != id_col.target]$id_col)
for (id in controls.id) {
controls[[j]] <- df[(id_col == id) & (t_col == yy)]$y_col
j <- j + 1
}
# check whether problem undetermined
if (length(controls[[1]]) < length(controls)) {
stop("Problem undetermined: number of data points is smaller than number of weights")
}
# evaluating the quantile functions on the grid "evgrid":
controls.q <- matrix(0,nrow = length(evgrid), ncol=length(controls))
for (jj in 1:length(controls)){
# controls.q[,jj] <- mapply(myquant, evgrid, MoreArgs = list(X=controls[[jj]]))
controls.q[,jj] <- myQuant(controls[[jj]], evgrid, qmethod)
}
# sample grid
grid <- list(grid.min = NA, grid.max = NA, grid.rand = NA, grid.ord = NA)
grid[c("grid.min", "grid.max", "grid.rand", "grid.ord")] <- getGrid(target, controls, G) # TODO: this can be done just once
if (yy <= T0) { # only get weights for pre-treatment periods
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M, qmethod=qmethod)
# obtaining the optimal weights for the mixture of distributions method
mixture <- DiSCo_mixture(controls, target, grid$grid.min, grid$grid.max, grid$grid.rand)
} else {
DiSCo_res_weights <- NA
mixture <- list(weights.opt = NA, distance.opt = NA, mean = NA)
}
#computing the target quantile function
target.q <- myQuant(target, evgrid, qmethod)
unlink("vignettes/Dube2019_cache", recursive = TRUE)
devtools::build()
unlink("vignettes/Dube2019_cache", recursive = TRUE)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
fldr <- here::here("R/")
sapply(paste0(fldr,list.files(fldr)), source)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
# library(data.table)
fldr <- here::here("R/")
sapply(paste0(fldr,list.files(fldr)), source)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
# library(pracma)
# library(quadprog)
# library(parallel)
library(data.table)
fldr <- here::here("R/")
sapply(paste0(fldr,list.files(fldr)), source)
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
df <- dube
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
M = 1000
G = 1000
num.cores = 5
permutation = FALSE
CI = TRUE
boots = 1000
cl = 0.95
CI_periods = NULL
CI_placebo=TRUE
graph = TRUE
qmethod=NULL
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
# create a column for the normalized time period
t_min <- min(df$time_col)
df[, t_col := time_col - t_min + 1]
T0 <- unique(df[time_col == t0]$t_col)  - 1
T_max <- max(df$t_col)
# create a list to store the results for each period
results.periods <- list()
evgrid = seq(from=0,to=1,length.out=M+1)
# run the main function in parallel for each period
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M,
G = G, T0 = T0, mc.cores = num.cores, qmethod=qmethod)
# turn results.periods into a named list where the name is the period
names(results.periods) <- as.character(periods)
yy <- 1
# target
target <- df[(id_col == id_col.target) & (t_col == yy)]$y_col
# generate list where each element contains a list of all micro-level outcomes for a control unit
controls <- list()
j <- 1
controls.id <- unique(df[id_col != id_col.target]$id_col)
for (id in controls.id) {
controls[[j]] <- df[(id_col == id) & (t_col == yy)]$y_col
j <- j + 1
}
# check whether problem undetermined
if (length(controls[[1]]) < length(controls)) {
stop("Problem undetermined: number of data points is smaller than number of weights")
}
# evaluating the quantile functions on the grid "evgrid":
controls.q <- matrix(0,nrow = length(evgrid), ncol=length(controls))
for (jj in 1:length(controls)){
# controls.q[,jj] <- mapply(myquant, evgrid, MoreArgs = list(X=controls[[jj]]))
controls.q[,jj] <- myQuant(controls[[jj]], evgrid, qmethod)
}
# sample grid
grid <- list(grid.min = NA, grid.max = NA, grid.rand = NA, grid.ord = NA)
grid[c("grid.min", "grid.max", "grid.rand", "grid.ord")] <- getGrid(target, controls, G) # TODO: this can be done just once
if (yy <= T0) { # only get weights for pre-treatment periods
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M, qmethod=qmethod)
# obtaining the optimal weights for the mixture of distributions method
mixture <- DiSCo_mixture(controls, target, grid$grid.min, grid$grid.max, grid$grid.rand)
} else {
DiSCo_res_weights <- NA
mixture <- list(weights.opt = NA, distance.opt = NA, mean = NA)
}
#computing the target quantile function
target.q <- myQuant(target, evgrid, qmethod)
yy <- 1
# target
target <- df[(id_col == id_col.target) & (t_col == yy)]$y_col
# generate list where each element contains a list of all micro-level outcomes for a control unit
controls <- list()
j <- 1
controls.id <- unique(df[id_col != id_col.target]$id_col)
for (id in controls.id) {
controls[[j]] <- df[(id_col == id) & (t_col == yy)]$y_col
j <- j + 1
}
# check whether problem undetermined
if (length(controls[[1]]) < length(controls)) {
stop("Problem undetermined: number of data points is smaller than number of weights")
}
# evaluating the quantile functions on the grid "evgrid":
controls.q <- matrix(0,nrow = length(evgrid), ncol=length(controls))
for (jj in 1:length(controls)){
# controls.q[,jj] <- mapply(myquant, evgrid, MoreArgs = list(X=controls[[jj]]))
controls.q[,jj] <- myQuant(controls[[jj]], evgrid, qmethod)
}
# sample grid
grid <- list(grid.min = NA, grid.max = NA, grid.rand = NA, grid.ord = NA)
grid[c("grid.min", "grid.max", "grid.rand", "grid.ord")] <- getGrid(target, controls, G) # TODO: this can be done just once
if (yy <= T0) { # only get weights for pre-treatment periods
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M, qmethod=qmethod)
# obtaining the optimal weights for the mixture of distributions method
mixture <- DiSCo_mixture(controls, target, grid$grid.min, grid$grid.max, grid$grid.rand)
} else {
DiSCo_res_weights <- NA
mixture <- list(weights.opt = NA, distance.opt = NA, mean = NA)
}
#computing the target quantile function
target.q <- myQuant(target, evgrid, qmethod)
# run the main function in parallel for each period
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M,
G = G, T0 = T0, mc.cores = num.cores, qmethod=qmethod)
# turn results.periods into a named list where the name is the period
names(results.periods) <- as.character(periods)
unlink("vignettes/Dube2019_cache", recursive = TRUE)
knit_with_parameters("~/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCo/vignettes/Dube2019.Rmd")
?norm
devtools::install()
unlink("vignettes/Dube2019_cache", recursive = TRUE)
?runif
?cvxr_norm
?%*%
?'%*%'
devtools::install()
unlink("vignettes/Dube2019_cache", recursive = TRUE)
?Variable
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
library(pracma)
library(quadprog)
library(parallel)
library(stats)
library(data.table)
library(CVXR)
library(DiSCo)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
df <- data.table::copy(dube)
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
df <- data.table::copy(dube)
df
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
# if the below gives issues it's the knit cache...
results <- DiSCo::DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_periods = NULL, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
devtools::install()
devtools::install()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = FALSE, boots = 1000, cl = 0.95, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
library(pracma)
library(quadprog)
library(parallel)
library(stats)
library(data.table)
library(CVXR)
library(DiSCo)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)
data("dube")
head(dube)
id_col.target <- 2
t0 <- 2003
df <- data.table::copy(dube)
df
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 5, permutation = FALSE,
CI = FALSE, boots = 1000, cl = 0.95, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
# if the below gives issues it's the knit cache...
results <- DiSCo(df, id_col.target, t0, M = 1000, G = 1000, num.cores = 1, permutation = FALSE,
CI = TRUE, boots = 1000, cl = 0.95, CI_placebo=TRUE, graph = TRUE, qmethod=NULL)
devtools::install()
