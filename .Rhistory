num.redraws=boots
evgrid = evgrid
DSC_res2.CI <- mclapply.hack(1:num.redraws, DiSCo_CI_iter, controls=controls, bc=bc,
weights=weights, cl=cl, evgrid = evgrid, mc.cores=mc.cores)
DSC_res2.CI <- matrix(unlist(DSC_res2.CI), ncol=length(evgrid), nrow=num.redraws)
CI.u <- apply(DSC_res2.CI,2,stats::quantile, probs=cl+(1-cl)/2)
CI.l <- apply(DSC_res2.CI,2,stats::quantile, probs=(1-cl)/2)
CI.u
CI.l
DSC_res2.CI
DSC_res2.CI[[1]]
DSC_res2.CI[1,]
DSC_res2.CI[2,]
DSC_res2.CI[500,]
dim(CI.u)
length(CI.u)
# DiSCo_CI_iter(1, controls=controls, bc=bc, weights=weights, cl=cl, evgrid = evgrid)
# obtain the cl% confidence interval
CI.u <- apply(DSC_res2.CI,2,quantile, probs=cl+(1-cl)/2)
CI.u
weights
CI.u
bc
dim(DSC_res2.CI)
?apply
length(CI.u)
# DiSCo_CI_iter(1, controls=controls, bc=bc, weights=weights, cl=cl, evgrid = evgrid)
# obtain the cl% confidence interval
CI.u <- apply(DSC_res2.CI,1,stats::quantile, probs=cl+(1-cl)/2)
length(CI.u)
DSC_res2.CI <- mclapply.hack(1:num.redraws, DiSCo_CI_iter, controls=controls, bc=bc,
weights=weights, cl=cl, evgrid=evgrid, mc.cores=mc.cores)
test <- matrix(unlist(DSC_res2.CI)) # , ncol=length(evgrid), nrow=num.redraws)
dim(test)
test
DSC_res2.CI
length(DSC_res2.CI)
test <- matrix(unlist(DSC_res2.CI), ncol=length(evgrid), nrow=num.redraws)
test[1,]
test[1,] == DSC_res2.CI[[1]]
DSC_res2.CI[[1]]
test[1,] == matrix(DSC_res2.CI[[1]])
test[1,] == matrix(unlist(DSC_res2.CI[[1]]))
test[1,] == matrix(unlist(DSC_res2.CI[[1]]))
matrix(unlist(DSC_res2.CI[[1]]))
DSC_res2.CI
mat <- do.call("cbind",DSC_res2.CI)
mat
dim(mat)
flattened_vector <- sapply(DSC_res2.CI, c)
test <- matrix(flatten_vector, ncol=length(evgrid), nrow=num.redraws, byrow=TRUE)
test <- matrix(flattened_vector, ncol=length(evgrid), nrow=num.redraws, byrow=TRUE)
test
test[1,]
dim(test)
test[,1]
test[1,]
result_matrix <- do.call(DSC_res2.CI, my_list)
result_matrix <- do.call(rbind, DSC_res2.)
result_matrix <- do.call(rbind, DSC_res2.CI)
result_matrix
result_matrix[1]
matrix(result_matrix)
dim(result_matrix)
DSC_res2.CI
type(DSC_res2.CI)
typeof(DSC_res2.CI)
DSC_res2.CI[,1]
DSC_res2.CI[[1]]
extracted_values <- sapply(DSC_res2.CI, function(x) x[, 1])
extracted_values <- sapply(DSC_res2.CI, function(x) x$barycenter)
result_matrix <- matrix(extracted_values, nrow=length(evgrid), ncol=num.redraws, byrow=TRUE)
result_matrix
dim(result_matrix)
result_matrix[,1]
result_matrix[,1] == DSC_res2.CI[[1]]$barycenter
DSC_res2.CI[[1]]$barycenter
result_matrix[,1] == DSC_res2.CI[[1]]$barycenter
result_matrix[,1]
extracted_values
dim(extracted_values)
extracted_values[,1]
extracted_values[,1] == DSC_res2.CI[[1]]$barycenter
test <- sapply(DSC_res2.CI, function(x) x$barycenter)
# DiSCo_CI_iter(1, controls=controls, bc=bc, weights=weights, cl=cl, evgrid = evgrid)
# obtain the cl% confidence interval
CI.u <- apply(test,2,stats::quantile, probs=cl+(1-cl)/2)
CI.u
# DiSCo_CI_iter(1, controls=controls, bc=bc, weights=weights, cl=cl, evgrid = evgrid)
# obtain the cl% confidence interval
CI.u <- apply(test,1,stats::quantile, probs=cl+(1-cl)/2)
CI.u
bc
CI.u
bc[99]
bc[99.]
bc[,99]
bc[[99]]
bc
bc[1,]
length(bc)
bc$barycenter[1,]
bc$barycenter[99,]
CI.u[99]
CI.u[500]
bc$barycenter[500,]
CI.l[500]
CI.l[501]
bc$barycenter[501,]
CI.u[501]
CI.l[502]
CI.l <- apply(test,1,stats::quantile, probs=(1-cl)/2)
CI.l[502]
se <- apply(DSC_res2.CI,1,sd)
DSC_res2.CI <- sapply(DSC_res2.CI, function(x) x$barycenter)
se <- apply(DSC_res2.CI,1,sd)
se
devtools::load_all()
# Load required packages
packages_load <- c("haven", "base", "data.table", "latex2exp", "CVXR",  # used to compute the weights using the alternative using mixtures of CDF
"here", "dplyr", "pracma", "quadprog", "R.utils", "foreach")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
# Set the working directory
setwd(file.path(here::here()))
#####
# Values to be chosen by the researcher
M <- 1000 # draws for samples to compute the respective integrals.
fips.target <- 2 # target state is AK
#####
# Required functions
# loading the function for computing the optimal weights
source('R/DiSCo_weights_reg.R')
# loading the function for computing the barycenter and the donor distributions
source('R/DiSCo_bc.R')
# loading the function for performing the permutation test
source('R/DiSCo_per.R')
## function to compute quantile function
myquant <- function(X,q){
# sort if unsorted
if (is.unsorted(X)) X <- sort(X)
# compute empirical CDF
X.cdf <- 1:length(X) / length(X)
# obtain the corresponding empirical quantile
return(X[which(X.cdf >= q)[1]])
}
#####
# loading the data-set on minimum wage from Dube (2019) to obtain states that did not have a
# change of the minimum wage between 1998-2014
df <- read_dta(file.path("data", "mw_annual_lagsleads_1974_2014.dta"))
years.aff <- list()
for (ii in unique(df$state_fips)){
hh <- df$mw[df$state_fips==ii & df$year>=1996]
hh.diff <- diff(hh)
# getting the years for which the difference is 0
years.help <- df$year[df$state_fips==ii & df$year>=1996]
years.aff[[ii]] <- years.help[which(hh.diff==0)]
}
# AK is our treated unit. We want states that did not have a change between 1998 and 2003/2004
states.aff <- list()
for (ii in 1:length(years.aff)){
dummyvec <- c(1998,1999,2000,2001,2002,2003,2004)
test <- intersect(dummyvec,years.aff[[ii]])
states.aff[[ii]] <- FALSE
if (length(dummyvec) == length(test)){
if (all(dummyvec == test) == TRUE) {
states.aff[[ii]] <- TRUE
}
}
}
control.states <- which(states.aff == TRUE)
# dropping the data-set for obtaining the control states and loading the actual data
rm(df, states.aff,years.aff, dummyvec,hh,hh.diff,ii,test,years.help)
# load the data from the stata file
fn <- file.path("data", "march_regready_1996.dta")
df <- read_dta(fn)  %>% setDT()
# only considering individuals under the age of 65
df <- df[demgroup1 == 1]
df <- df[year %between% c(1998, 2004)]
summary(df$adj0contpov)
# collapsing the data
df[,.(adj0contpov=mean(adj0contpov)),
by=c('state_fips', 'year', 'hhseq')]
summary(df$adj0contpov)
df <- df[, c("adj0contpov", "state_fips", "year")]
saveRDS(df, file.path("data", "march_regready_1996.rds"), compress = TRUE)
results.over.years <- list()
df <- df[state_fips %in% c(fips.target, control.states)]
df[, id_col := state_fips]
# create t_col starting at 1 and increasing by 1 for each year
df[, time_col := year]
df[, y_col := adj0contpov]
# other required arguments
id_col.target = fips.target
t0 = 2003
M = 1000
G = 1000
num.cores = parallel::detectCores() - 1
permutation = FALSE
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
# create a column for the normalized time period
t_min <- min(df$time_col)
df[, t_col := time_col - t_min + 1]
T0 <- unique(df[time_col == t0]$t_col) - 1
T_max <- max(df$t_col)
# create a list to store the results for each period
results.periods <- list()
evgrid = seq(from=0,to=1,length.out=M+1)
# run the main function in parallel for each period
start_time <- Sys.time()
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M, G = G, T0 = T0, mc.cores = num.cores)
end <- Sys.time()
print(end - start_time)
# turn results.years into a named list where the name is the year
names(results.periods) <- as.character(periods)
sort(results.periods$`1`$DiSCo_weights)
T0
# run the main function in parallel for each period
start_time <- Sys.time()
set.seed(1860)
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M, G = G, T0 = T0, mc.cores = num.cores)
end <- Sys.time()
print(end - start_time)
# turn results.years into a named list where the name is the year
names(results.periods) <- as.character(periods)
sort(results.periods$`1`$DiSCo_weights)
df$t_col
yy <- 1
# target
target <- df[id_col == id_col.target & t_col == yy]$y_col
# generate list where each element contains a list of all micro-level outcomes for a control unit
controls <- list()
j <- 1
controls.id <- unique(df[id_col != id_col.target]$id_col)
for (id in controls.id) {
controls[j] <- list(df[id_col == id & t_col == yy]$y_col)
j <- j + 1
}
# check whether problem undetermined
if (length(controls[1][[1]]) < length(controls)) {
stop("Problem undetermined: number of data points is smaller than number of weights")
}
# evaluating the quantile functions on the grid "evgrid":
controls.q <- matrix(0,nrow = length(evgrid), ncol=length(controls))
for (jj in 1:length(controls)){
controls.q[,jj] <- mapply(myquant, evgrid, MoreArgs = list(X=controls[[jj]]))
}
controls
length(controls)
controls[[1]]
target
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M)
DiSCo_res_weights
#' Function for obtaining the weights in the DiSCo method at every time period
#' Estimate the optimal weights for the distributional synthetic controls method by
#' solving the convex minimization problem in Eq. (2) in \insertref{gunsilius2020distributional}{DiSCo}
#' using a regression of the simulated target quantile on the simulated control quantiles, as in Eq. (3),
#' \eqn{\underset{\vec{\lambda} \in \Delta^J}{\operatorname{argmin}}\left\|\mathbb{Y}_t \vec{\lambda}_t-\vec{Y}_{1 t}\right\|_2^2}.
#' For the constrained optimization we rely on the package pracma
#' the control distributions can be given in list form, where each list element contains a
#' vector of observations for the given control unit, in matrix form;
#' in matrix- each column corresponds to one unit and each row is one observation.
#' The list-form is useful, because the number of draws for each control group can be different.
#' The target must be given as a vector.
#'
#' @param controls List with matrices of control distributions
#' @param target Matrix containing the target distribution
#' @param M Optional integer, number of draws from the uniform distribution for approximating the integral. See section 3.1 in the paper.
#'
#' @return
DiSCo_weights_reg <- function(controls,target, M = 500){
if (!is.vector(target)){
stop("Target needs to be given as a vector.")
}
if (!is.list(controls) && !is.matrix(controls)){
stop ("Controls need to be given in either list-form or matrix form.")
}
# if the controls are given in matrix form we turn them into a list
if (is.matrix(controls)) {
controls.h <- list()
for (ii in 1:ncol(controls)){
controls.h[[ii]] <- as.vector(controls[,ii])
}
controls <- controls.h
}
# M is the number of draws from the uniform distribution for approximating the integral
## Sampling from this quantile function M times
Mvec <- runif(M, min = 0, max = 1)
controls.s <- matrix(0,nrow = M, ncol = length(controls))
for (jj in 1:length(controls)){
controls.s[,jj] <- mapply(myquant, Mvec, MoreArgs=list(X=controls[[jj]]))
}
target.s <- matrix(0, nrow = M, ncol=1)
target.s[,1] <- mapply(myquant, Mvec, MoreArgs=list(X=target))
## Solving the optimization using constrained linear regression
# the equality constraints
Aequ <- matrix(rep(1,length(controls)),nrow=1, ncol=length(controls))
# if the values in controls.s and target.s are too large it can happen that we run into
# overflow errors. For this reason we scale both the vector and the matrix by the Frobenius
# norm of the matrix
sc <- norm(controls.s,"2")
# solve with the pracma package, which runs FORTRAN under the hood so it is fast
weights.opt <- pracma::lsqlincon(controls.s/sc,target.s/sc, A=NULL,b=NULL,
Aeq = Aequ, # LHS of equality constraint: matrix of 1s, need coefficients to lie in unit simplex
beq = 1, 0, 1 # RHS of equality constraint, unit simplex
)
return(weights.opt)
}
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M)
DiSCo_res_weights
sort(DiSCo_res_weights)
length(controls)
length(controls[[1]])
controls[[1]]
length(target)
target
length(target)
packageVersion("pracma")
set.seed(1860)
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M)
controls
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M)
DiSCo_res_weights
library(quadprog)
# obtaining the optimal weights for the DiSCo method
DiSCo_res_weights <- DiSCo_weights_reg(controls, as.vector(target), M)
DiSCo_res_weights
controls <- controls
target <- as.vector(target)
if (!is.vector(target)){
stop("Target needs to be given as a vector.")
}
if (!is.list(controls) && !is.matrix(controls)){
stop ("Controls need to be given in either list-form or matrix form.")
}
# if the controls are given in matrix form we turn them into a list
if (is.matrix(controls)) {
controls.h <- list()
for (ii in 1:ncol(controls)){
controls.h[[ii]] <- as.vector(controls[,ii])
}
controls <- controls.h
}
# M is the number of draws from the uniform distribution for approximating the integral
## Sampling from this quantile function M times
Mvec <- runif(M, min = 0, max = 1)
controls.s <- matrix(0,nrow = M, ncol = length(controls))
for (jj in 1:length(controls)){
controls.s[,jj] <- mapply(myquant, Mvec, MoreArgs=list(X=controls[[jj]]))
}
target.s <- matrix(0, nrow = M, ncol=1)
target.s[,1] <- mapply(myquant, Mvec, MoreArgs=list(X=target))
## Solving the optimization using constrained linear regression
# the equality constraints
Aequ <- matrix(rep(1,length(controls)),nrow=1, ncol=length(controls))
# if the values in controls.s and target.s are too large it can happen that we run into
# overflow errors. For this reason we scale both the vector and the matrix by the Frobenius
# norm of the matrix
sc <- norm(controls.s,"2")
Aequ
sc
controls.s
controls[[1]]
controls[[2]]
controls[[9]]
Mbec
Mvec
devtools::load_all()
# make sure we have a data table
df <- as.data.table(df)
# check the inputs
checks(df, id_col.target, t0, M, G, num.cores, permutation)
# create a column for the normalized time period
t_min <- min(df$time_col)
df[, t_col := time_col - t_min + 1]
T0 <- unique(df[time_col == t0]$t_col)  - 1
T_max <- max(df$t_col)
# create a list to store the results for each period
results.periods <- list()
evgrid = seq(from=0,to=1,length.out=M+1)
# run the main function in parallel for each period
start_time <- Sys.time()
set.seed(1860)
periods <- sort(unique(df$t_col)) # we call the iter function on all periods, but won't calculate weights for the post-treatment periods
results.periods <- mclapply.hack(periods, DiSCo_iter, df, evgrid, id_col.target = id_col.target, M = M, G = G, T0 = T0, mc.cores = num.cores)
end <- Sys.time()
print(end - start_time)
# turn results.years into a named list where the name is the year
names(results.periods) <- as.character(periods)
#####
#obtaining the weights as a uniform mean over all time periods
Weights_DiSCo_avg <- results.periods$`1`$DiSCo_weights
Weights_mixture_avg <- results.periods$`1`$mixture$weights
for (yy in 2:T0){
Weights_DiSCo_avg <- Weights_DiSCo_avg + results.periods[[yy]]$DiSCo_weights
Weights_mixture_avg <- Weights_mixture_avg + results.periods[[yy]]$mixture$weights
}
Weights_DiSCo_avg <- (1/T0) * Weights_DiSCo_avg
Weights_mixture_avg <- (1/T0) * Weights_mixture_avg
# calculating the counterfactual target distribution
bc <- lapply(seq(1:T_max), function(x) DiSCo_bc(results.periods[[x]]$controls$data, results.periods[[x]]$controls.q, Weights_DiSCo_avg, evgrid))
names(bc) <- t_min  + seq(1:T_max) - 1
graph <- TRUE
controls_per <- lapply(seq(1:T_max), function(x) results.periods[[x]]$controls$data)
target_per <- lapply(seq(1:T_max), function(x) results.periods[[x]]$target$data)
controls.q <- lapply(seq(1:T_max), function(x) results.periods[[x]]$controls.q)
perm <- DiSCo_per(c_df=controls_per, t_df=target_per, controls.q=controls.q, T0=T0, weights=Weights_DiSCo_avg, num_cores=num.cores, evgrid=evgrid,
graph=graph)
#' @title DiSCo_per
#'
#' @description Function to implement permutation test for Distributional Synthetic Controls
#' @details This program iterates through all units and computes the optimal weights on the other units
#' for replicating the unit of iteration's outcome variable, assuming that it is the treated unit.
#' See Algorithm 1 in \insertref{gunsilius2020distributional}{DiSCo} for more details.
#'
#' @param c_df List with matrices of control distributions
#' @param t_df Matrix containing the target distribution
#' @param T0 Integer indicating first year of treatment as counted from 1 (e.g, if treatment year 2002 was the 5th year in the sample, this parameter should be 5).
#' @param ww Optional vector of weights indicating the relative importance of each time period. If not specified, each time period is weighted equally.
#' @param peridx Optional integer indicating number of permutations. If not specified, by default equal to the number of units in the sample.
#' @param evgrid Optional vector containing an evenly spaced grid on [0,1] on which the quantile function for the control units will be evaulated.
#' By default, a grid of 100 points is used.
#' @param graph Boolean indicating whether to plot graphs
#' @param y_name Y axis label of the graph
#' @param x_name X axis label of the graph
#' @param num_cores Integer, number of cores to use for parallel computation. Set to 1 by default (sequential computation), this can be very slow!
#' @return List of matrices containing synthetic time path of the outcome variable
#' for the target unit together with the time paths of the control units
DiSCo_per <- function(c_df, t_df, controls.q, T0, ww=0, peridx=0, evgrid=seq(from=0, to=1, length.out=101),
graph=TRUE, num_cores = 1, redo_weights=FALSE, weights=NULL){
#----------------------------------------#
# target
#----------------------------------------#
if (redo_weights) {
#calculate lambda_t for t<=T0
lambda_t=list()
lambda_t <- parallel::mclapply.hack(seq_len(T0), function(t) {
DiSCo_weights_reg(c_df[[t]], as.vector(t_df[[t]]), 1000)
}, mc.cores = num_cores)
#calculate the average optimal lambda
if (length(ww)==1){
w_t=rep(1/T0, T0)
lambda.opt=matrix(unlist(lambda_t),ncol=T0)%*%w_t
} else{
lambda.opt=matrix(unlist(lambda_t),ncol=T0)%*%ww
}
} else if (is.null(weights)){
stop("Please provide either weights or set redo_weights to TRUE")
} else {
lambda.opt=weights
}
#calculate the barycenters for each period
bc_t=list()
bc_t <- mclapply.hack(1:length(c_df), function(x) {
DiSCo_bc(c_df[[x]], controls.q[[x]], lambda.opt, evgrid)
}, mc.cores = num_cores)
#computing the target quantile function
target_q=list()
for (t in 1:length(t_df)){
target_q[[t]] <- mapply(myquant, evgrid, MoreArgs = list(X=t_df[[t]]))
}
#squared Wasserstein distance between the target and the corresponding barycenter
distt=c()
for (t in 1:length(c_df)){
distt[t]=mean((bc_t[[t]][[2]]-target_q[[t]])**2)
}
#----------------------------------------#
# permutation
#----------------------------------------#
#default permute all controls
if (peridx==0){
peridx=1:length(c_df[[1]])
}
cat("Starting permutation test...")
distp <- mclapply.hack(seq_len(length(peridx)), function(idx) {
DiSCo_per_iter(c_df=c_df, c_df.q=controls.q, t_df=t_df, T0=T0, ww=ww, peridx=peridx, evgrid=evgrid, idx=idx)
}, mc.cores = num_cores)
cat('Permutation finished!')
# Convert the list to a nested list (the parallelization messes up the output of foreach when choosing .combine = list)
distp <- matrix(unlist(distp), ncol = length(c_df), byrow = TRUE)
distp <- split(distp, seq_len(nrow(distp)))
#default plot all squared Wasserstein distances
if (graph==TRUE){
plot(distt, xlab='',ylab='', type='l', lwd=2)
for (i in 1:length(distp)){
lines(1:length(c_df), distp[[i]], col='grey', lwd=1)
}
abline(v=T0, lty = 2)
legend("topleft",legend = c("Target", "Control"),
col=c("black", "grey"),
lty= c(1,1), lwd = c(2,2), cex = 1.5)
title(ylab="Squared Wasserstein distance", line=2.5, cex.lab=1.5)
title(xlab="Time periods", line=3, cex.lab=1.5)
}
return(list(target.dist=distt, control.dist=distp))
}
controls_per <- lapply(seq(1:T_max), function(x) results.periods[[x]]$controls$data)
target_per <- lapply(seq(1:T_max), function(x) results.periods[[x]]$target$data)
controls.q <- lapply(seq(1:T_max), function(x) results.periods[[x]]$controls.q)
perm <- DiSCo_per(c_df=controls_per, t_df=target_per, controls.q=controls.q, T0=T0, weights=Weights_DiSCo_avg, num_cores=num.cores, evgrid=evgrid,
graph=graph)
saveRDS("test.rds")
saveRDS(file="test.rds")
