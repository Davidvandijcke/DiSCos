---
title: "Empirical Application in Gunsilius (2023)"
author: "David Van Dijcke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Empirical Application in Gunsilius (2023)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/REFERENCES.bib
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    comment = "#",
    echo=TRUE,
  error = FALSE,
  tidy = FALSE,
  cache = FALSE,
  collapse = TRUE,
  out.width = '100%',
  dpi = 144
)
```

```{r, include=FALSE}

# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)# library(CVXR)
library(pracma)
library(quadprog)
library(parallel)
library(stats)
library(data.table)
library(CVXR)
library(DiSCo)
library(ggplot2)
# fldr <- here::here("R/")
# sapply(paste0(fldr,list.files(fldr)), source)
```

# Introduction

This vignette demonstrates how to use the DiSCo package by way of the
empirical application in @gunsilius2023distributional, which is based on
@dube2019minimum. We illustrate the use of the two main functions: 1)
`DiSCo`, which estimates the raw distributional counterfactuals,
computes confidence intervals, and optionally performs a permutation
test; and 2) `DiSCoTEA`, the "Treatment Effect Aggregator", which takes
in the distributional counterfactuals and computes aggregate treatment
effects using a user-specified aggregation statistic.

# Distributional Synthetic Controls

We briefly review the main idea behind Distributional Synthetic
Controls. Denote $Y_{jt,N}$ the outcome of group $j$ in time period $t$
in the absence of an intervention. Also denote $Y_{jt,I}$ the outcome in
the presence of an intervention at time $t > T_0$. Denote the quantile
function, $$
F^{-1}(q):=\inf _{y \in \mathbb{R}}\{F(y) \geq q\}, \quad q \in(0,1),
$$ where $F(y)$ is the corresponding cumulative distribution function.
One unit $j=1$ has received treatment, while the other units
$j=2,\ldots,J$ have not. Then the goal is to estimate the counterfactual
quantile function $F_{Y_{1 t}, N}^{-1}$ of the treated unit had it not
received treatment by an optimally weighted average of the control
units' quantile functions, $$
F_{Y_{1 t}, N}^{-1}(q)=\sum_{j=2}^{J+1} \lambda_j^* F_{Y_{j t}}^{-1}(q) \quad \text { for all } q \in(0,1)
$$\
In practice, we do this by solving the following optimization problem:
$$
\vec{\lambda}_t^*=\underset{\vec{\lambda} \in \Delta^J}{\operatorname{argmin}} \int_0^1\left|\sum_{j=2}^{J+1} \lambda_j F_{Y_{j t}}^{-1}(q)-F_{Y_{1 t}}^{-1}(q)\right|^2 d q
$$ which gives an optimal weight for each unit-period combination. This problem can be
solved by a simple weighted regression, which is implemented in the
`DiSCo_weights_reg` function. To obtain the overall optimal weights
$\vec{\lambda}^*$, we take the average of the optimal weights across all
periods.

# Illustration

The data from @dube2019minimum is available in the package. We load it
here:

```{r}
data("dube")
head(dube)
```

To learn more about the data, just type `?dube` in the console. We have
already renamed the outcome, id, and time variables to `y_col`,
`id_col`, and `time_col`, respectively, which is required before passing
the dataframe to the DiSCo command. We also need to set the two
following parameters:

```{r}
id_col.target <- 2
t0 <- 2003
```

which indicate the id of the treated unit and the time period of the
intervention, respectively. We can now run the DiSCo command:

```{r}
# if the below gives issues it's the knit cache...

disco <- DiSCo(dube, id_col.target, t0, G = 1000, num.cores = 2, permutation = FALSE,
                 CI = TRUE, boots = 500, graph = TRUE)
```

where we have chosen a grid (`G`) of 1000 quantile and opted for parallel
computation with 2 cores to speed up the permutation test and confidence
intervals (`CI` is set to `TRUE`), which we calculate using 500 resamples (`boots`). 

We also set `permutation` to `TRUE`, which runs the permutation test described in the paper (see `?DiSCo_per` for more details). 
This will allow us to inspect the permutation inference results below. Already, by setting `graph` to `TRUE`,
the function displayed a plot of the full distribution of permutation tests. The black solid line shows the
fit of the "true" synthetic control. The fact that it does not diverge stronger than the other lines in gray after 
treatment suggests that the treatment had no effect. 


For context, the y-axis is the squared Wasserstein distance between the counterfactual and observed quantile functions,
$$
d_{t t}^2:=\int_0^1\left|F_{Y_{u t, N}}^{-1}(q)-F_{Y_{u t}}^{-1}(q)\right|^2 d q
$$
and, as in @abadie2010synthetic we take the ratio of post- to pre-intervention Wasserstein distance to account for variation in the pre-treatment fit across placebo tests,
\[
r_j=\frac{R_j\left(T_0+1, T\right)}{R_j\left(1, T_0\right)}
\]
and calculate the p-value for the permutation test as,
\[
p=\frac{1}{J+1} \sum_{j=1}^{J+1} H\left(r_j-r_1\right),
\]
where $H(x) $ is the Heaviside function which is 1 if $x\geq 0$ and 0 otherwise. 
This p-value gives the probability of observing a placebo test with a larger ratio than the true treatment effect.
It can be retrieved by calling
```{r}
summary(disco$perm)
```




We can use the DiSCo Treatment Effect Aggregator (`DiSCoTEA`) function
to aggregate the resulting counterfactual quantile functions into
various treatment effect measures. For example, we can calculate the
average treatment effect (ATE) as follows:

```{r}
discot <- DiSCoTEA(disco,  agg="quantileDiff", graph=TRUE)
summary(discot)

```


# Permutation test

Similar to the canonical permutation test in @abadie2010synthetic, we
can carry out a distributional permutation test. The idea is entirely
analogous: we iteratively reassign the treatment to units in the set of
controls and estimate "placebo" quantile functions. Then we calculate
the squared Wasserstein error of the entire distribution as, $$
d_{t t}^2:=\int_0^1\left|F_{Y_{u t, N}}^{-1}(q)-F_{Y_{u t}}^{-1}(q)\right|^2 d q
$$ for each time period. The `permutation` option in the `DiSCo`
function allows one to plot the time evolution of these terms for all
units by setting `graph = TRUE`, to visually inspect the abnormality of
the "true" treatment effects. We also calculate the p-value of the test
as, $$
p_t=\frac{r\left(d_{1 t}\right)}{J+1}
$$ where $r(d_{1t})$ is the rank of the true treatment effect in the
distribution of placebo effects.

